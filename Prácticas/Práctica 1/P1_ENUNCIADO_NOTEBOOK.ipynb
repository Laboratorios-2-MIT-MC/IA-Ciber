{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **INTELIGENCIA ARTIFICIAL APLICADA A LA CIBERSEGURIDAD**\n",
        "## **PRÁCTICA P1 - BLOQUE I**\n",
        "\n",
        "**INSTRUCCIONES / RECOMENDACIONES**\n",
        "\n",
        "- Se recomienda leer con detalle la descripción de cada una de las celdas.\n",
        "- Las celdas que ya tienen código, se deberán ejecutar directamente.\n",
        "- Las celdas que están vacías, se completarán con la implementación requerida en el notebook.\n",
        "- No se incluirán más celdas de las establecidas en el presente notebook, por lo que la solución al mismo deberá implementarse exclusivamente en las celdas vacías.\n",
        "- Scikit-Learn es un paquete muy útil para las operaciones de preprocesamiento de los datos, como estandarización, normalización, codificación y performance de los modelos.\n",
        "- Si ves que un apartado es complejo, intenta escribir y ejecutarlo de forma simplificada (por ejemplo, con menos layers o con menos features) y después vaya ampliándolo.\n",
        "- La entrega se realizará vía Moodle. Será necesario subir la solución a este notebook con el nombre: **NOMBRE_GRUPO.ipynb**\n",
        "\n",
        "- **Fecha de Publicación: 12/02/2024**\n",
        "- **Fecha de Entrega: 18/02/2024**\n",
        "- **Test: 19/02/2024**\n"
      ],
      "metadata": {
        "id": "XFJHhP0CNCq0"
      },
      "id": "XFJHhP0CNCq0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de librerías"
      ],
      "metadata": {
        "id": "6SPLwA4uN8oP"
      },
      "id": "6SPLwA4uN8oP"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "731a1aa0-a5ac-4398-a5a0-834a71fd0e17",
      "metadata": {
        "id": "731a1aa0-a5ac-4398-a5a0-834a71fd0e17",
        "outputId": "15ecdd48-3bf7-4720-c821-a79261f249e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga dataset\n",
        "\n",
        "Se va a usar un dataset ('**creditcard.csv**') que contiene información sobre transacciones económicas, en donde cada una de las transacciones está etiquetada como caso de fraude o caso de no fraude.\n",
        "El objetivo será construir un modelo MLP que permita detectar transacciones fraudulentas, tratándose por lo tanto de un problema de clasificación binaria.\n",
        "\n",
        "El dataset se puede descargar desde el siguiente enlace:\n",
        "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download\n"
      ],
      "metadata": {
        "id": "hESY4dJdOanL"
      },
      "id": "hESY4dJdOanL"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b33f2378-9189-424f-9bc7-5d4bcfdb8358",
      "metadata": {
        "id": "b33f2378-9189-424f-9bc7-5d4bcfdb8358",
        "outputId": "dfe79ba3-30db-485d-e9c1-ac64d3b9e742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-13 18:17:29--  https://github.com/Laboratorios-2-MIT-MC/IA-Ciber/raw/main/Pr%C3%A1cticas/Pr%C3%A1ctica%201/creditcard.zip\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Laboratorios-2-MIT-MC/IA-Ciber/main/Pr%C3%A1cticas/Pr%C3%A1ctica%201/creditcard.zip [following]\n",
            "--2024-02-13 18:17:30--  https://raw.githubusercontent.com/Laboratorios-2-MIT-MC/IA-Ciber/main/Pr%C3%A1cticas/Pr%C3%A1ctica%201/creditcard.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69155672 (66M) [application/zip]\n",
            "Saving to: ‘creditcard.zip’\n",
            "\n",
            "creditcard.zip      100%[===================>]  65.95M   359MB/s    in 0.2s    \n",
            "\n",
            "2024-02-13 18:17:33 (359 MB/s) - ‘creditcard.zip’ saved [69155672/69155672]\n",
            "\n",
            "Archive:  creditcard.zip\n",
            "  inflating: creditcard.csv          \n"
          ]
        }
      ],
      "source": [
        "# Download data from repository and unzip\n",
        "!wget https://github.com/Laboratorios-2-MIT-MC/IA-Ciber/raw/main/Pr%C3%A1cticas/Pr%C3%A1ctica%201/creditcard.zip -O creditcard.zip\n",
        "!unzip creditcard.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparación de los dataset de Train, Validation y Test\n",
        "\n",
        "Se deberán crear los dataset **train_loader**, **val_loader** y **test_loader** (Entrenamiento, validación y test), especificando el parámetro **batch_size** que se empleará posteriormente durante el entrenamiento."
      ],
      "metadata": {
        "id": "Mj7hRdl6PmmE"
      },
      "id": "Mj7hRdl6PmmE"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a9d5b021-9800-4976-9416-680beed2515e",
      "metadata": {
        "id": "a9d5b021-9800-4976-9416-680beed2515e",
        "outputId": "480ca74c-2634-4f6a-bb38-bdd558cc99fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    284315\n",
            "1       492\n",
            "Name: Class, dtype: int64\n",
            "TRAIN NO_FRAUD:  227459\n",
            "TRAIN FRAUD:     386\n",
            "VAL NO_FRAUD:  28419\n",
            "VAL FRAUD:     62\n",
            "TEST NO_FRAUD:  28437\n",
            "TEST FRAUD:     44\n",
            "# total samples: 284807\n",
            "# total TRAIN samples: 227845\n",
            "# total VAL samples: 28481\n",
            "# total TEST samples: 28481\n"
          ]
        }
      ],
      "source": [
        "# DEFINE & CREATE DATASET\n",
        "\n",
        "class CreditcardDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "\n",
        "        self.df = pd.read_csv(csv_file, sep=\",\")\n",
        "\n",
        "        y = self.df.iloc[:]['Class'].values\n",
        "        x = self.df.drop('Class', axis=1).values\n",
        "\n",
        "        self.X = torch.tensor(x, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(y)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        print(self.df.iloc[:]['Class'].value_counts())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "CC_dataset = CreditcardDataset(csv_file='/content/creditcard.csv')\n",
        "\n",
        "\n",
        "\n",
        "# PREPARE DATASETS\n",
        "# Dataset balanceado\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 512\n",
        "train_split = 0.8 # 80%\n",
        "val_split = 0.5 # 50% del resto\n",
        "\n",
        "train_size = int(train_split * len(CC_dataset))\n",
        "val_size = int(val_split * (len(CC_dataset) - train_size))\n",
        "test_size = len(CC_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(CC_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# WeightedRandomSampler for TRAIN\n",
        "class_0 = 0\n",
        "class_1 = 0\n",
        "train_labels = []\n",
        "for i in range(len(train_dataset)):\n",
        "    _, Y = train_dataset[i]\n",
        "\n",
        "    if Y==0:\n",
        "        class_0+=1\n",
        "    else:\n",
        "        class_1+=1\n",
        "\n",
        "    train_labels.append(Y)\n",
        "\n",
        "print ('TRAIN NO_FRAUD: ', class_0)\n",
        "print ('TRAIN FRAUD:    ', class_1)\n",
        "\n",
        "class_counts = [class_0, class_1]\n",
        "num_samples = sum(class_counts)\n",
        "labels = train_labels #train_dataset.Y #corresponding labels of samples\n",
        "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
        "train_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
        "\n",
        "# WeightedRandomSampler for VAL\n",
        "class_0 = 0\n",
        "class_1 = 0\n",
        "val_labels = []\n",
        "for i in range(len(val_dataset)):\n",
        "    _, Y = val_dataset[i]\n",
        "\n",
        "    if Y==0:\n",
        "        class_0+=1\n",
        "    else:\n",
        "        class_1+=1\n",
        "\n",
        "    val_labels.append(Y)\n",
        "\n",
        "print ('VAL NO_FRAUD: ', class_0)\n",
        "print ('VAL FRAUD:    ', class_1)\n",
        "\n",
        "class_counts = [class_0, class_1]\n",
        "num_samples = sum(class_counts)\n",
        "labels = val_labels\n",
        "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
        "val_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
        "\n",
        "# WeightedRandomSampler for TEST\n",
        "class_0 = 0\n",
        "class_1 = 0\n",
        "test_labels = []\n",
        "for i in range(len(test_dataset)):\n",
        "    _, Y = test_dataset[i]\n",
        "\n",
        "    if Y==0:\n",
        "        class_0+=1\n",
        "    else:\n",
        "        class_1+=1\n",
        "\n",
        "    test_labels.append(Y)\n",
        "\n",
        "print ('TEST NO_FRAUD: ', class_0)\n",
        "print ('TEST FRAUD:    ', class_1)\n",
        "\n",
        "class_counts = [class_0, class_1]\n",
        "num_samples = sum(class_counts)\n",
        "labels = test_labels #train_dataset.Y #corresponding labels of samples\n",
        "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
        "test_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                            #shuffle=True,\n",
        "                            num_workers=0,\n",
        "                            sampler=train_sampler\n",
        "                            )\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                            #shuffle=True,\n",
        "                            num_workers=0,\n",
        "                            sampler=val_sampler\n",
        "                            )\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                            #shuffle=True,\n",
        "                            num_workers=0,\n",
        "                            sampler=test_sampler\n",
        "                            )\n",
        "\n",
        "print (\"# total samples:\", len(CC_dataset))\n",
        "print (\"# total TRAIN samples:\", len(train_loader.sampler))\n",
        "print (\"# total VAL samples:\", len(val_loader.sampler))\n",
        "print (\"# total TEST samples:\", len(test_loader.sampler))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis del dataset\n",
        "\n",
        "Realice un análisis de las variables en el dataset.\n",
        "Sugerencias:\n",
        "- Histogramas de las variables.\n",
        "- Obtención de valores de centralidad y dispersión.\n",
        "- Detectar variables que no parezcan predecir el target o si presentan alta correlación con alguna otra de las features.\n",
        "\n"
      ],
      "metadata": {
        "id": "FwtMMeEKSymt"
      },
      "id": "FwtMMeEKSymt"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7E6AgtBTAII"
      },
      "id": "x7E6AgtBTAII",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8xwJ1lBhTEDS"
      },
      "id": "8xwJ1lBhTEDS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMXriuIITA3B"
      },
      "id": "xMXriuIITA3B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activación de GPU"
      ],
      "metadata": {
        "id": "8hfU5X-MQEv8"
      },
      "id": "8hfU5X-MQEv8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd2813b-1e3b-49b1-bf9d-93103b0cd843",
      "metadata": {
        "id": "ecd2813b-1e3b-49b1-bf9d-93103b0cd843"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación del modelo MLP\n",
        "\n",
        "Se creará un modelo MLP denominado **model** que implementará un clasificador binario para la detección de transacciones fraudulentas."
      ],
      "metadata": {
        "id": "k1zkZdxDQOC3"
      },
      "id": "k1zkZdxDQOC3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274dd20d-48ab-41da-918c-0020a6a47832",
      "metadata": {
        "id": "274dd20d-48ab-41da-918c-0020a6a47832"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definición de función de pérdida y optimizador\n",
        "\n",
        "Especificar el parámetro **lr** (learning rate) que se empleará durante el entrenamiento del modelo."
      ],
      "metadata": {
        "id": "HXZ9vpdDQkEY"
      },
      "id": "HXZ9vpdDQkEY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42be73e8-762e-4d90-a746-b52e9bb2163d",
      "metadata": {
        "id": "42be73e8-762e-4d90-a746-b52e9bb2163d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definición de funciones de Entrenamiento y Validación"
      ],
      "metadata": {
        "id": "sidnVK-xQxh-"
      },
      "id": "sidnVK-xQxh-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2233e97b-ddf5-48d3-85a9-9c669089886b",
      "metadata": {
        "id": "2233e97b-ddf5-48d3-85a9-9c669089886b"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, epoch):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "\n",
        "    for i, (X, Y) in enumerate(train_loader):\n",
        "\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, Y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        num_corrects = (torch.max(outputs, 1)[1].view(Y.size()).data == Y.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(Y)\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "          print (f'Epoch: {epoch+1}, Idx: ({i+1}/{len(train_loader)}), Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(train_loader), total_epoch_acc/len(train_loader)\n",
        "\n",
        "def eval_model(model, val_loader, criterion):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total_epoch_loss = 0\n",
        "  total_epoch_acc = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for i, (X, Y) in enumerate(val_loader):\n",
        "\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, Y)\n",
        "\n",
        "        num_corrects = (torch.max(outputs, 1)[1].view(Y.size()).data == Y.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(Y)\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_loader), total_epoch_acc/len(val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento del modelo\n",
        "\n",
        "Especificar el parámetro **epochs** que se utilizará durante el entrenamiento del modelo."
      ],
      "metadata": {
        "id": "tKuZg3k2RGAD"
      },
      "id": "tKuZg3k2RGAD"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8nUa81hRyze"
      },
      "id": "J8nUa81hRyze",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc99b0a-77b6-4210-abab-d76ba93e3dee",
      "metadata": {
        "id": "9fc99b0a-77b6-4210-abab-d76ba93e3dee"
      },
      "outputs": [],
      "source": [
        "train_loss_epochs = []\n",
        "val_loss_epochs = []\n",
        "train_acc_epochs = []\n",
        "val_acc_epochs = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, epoch)\n",
        "    val_loss, val_acc = eval_model(model, val_loader, criterion)\n",
        "\n",
        "    print(f'Epoch: [{epoch+1:02}/{epochs}], Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        "    train_loss_epochs.append(train_loss)\n",
        "    val_loss_epochs.append(val_loss)\n",
        "    train_acc_epochs.append(train_acc)\n",
        "    val_acc_epochs.append(val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mostrar la evolución del error y del accuracy durante el entrenamiento del modelo\n",
        "\n",
        "Para mostrar el error y accuracy del dataset de train y de val, haga uso de las listas creadas durante el entrenamiento del modelo (ver celda anterior)."
      ],
      "metadata": {
        "id": "bqd2c7G2ST12"
      },
      "id": "bqd2c7G2ST12"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExYVBlElSZPy"
      },
      "id": "ExYVBlElSZPy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oXWHHrqZSnbA"
      },
      "id": "oXWHHrqZSnbA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verificar el rendimiento del modelo creado\n",
        "\n",
        "Utilice el dataset de test para verificar el correcto rendimiento del modelo. Para ello puede emplear las métricas que considere oportunas, como por ejemplo Accuracy, Precision, Recall, F1-score o matriz de confusión."
      ],
      "metadata": {
        "id": "w9g--JPGSbR-"
      },
      "id": "w9g--JPGSbR-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dcc272a-434c-4169-b5fd-a12a93b93407",
      "metadata": {
        "id": "3dcc272a-434c-4169-b5fd-a12a93b93407"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c488a279-85fb-4e73-b11e-bb6cdb49ca69",
      "metadata": {
        "id": "c488a279-85fb-4e73-b11e-bb6cdb49ca69"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GSuDPWOeSk5f"
      },
      "id": "GSuDPWOeSk5f",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}