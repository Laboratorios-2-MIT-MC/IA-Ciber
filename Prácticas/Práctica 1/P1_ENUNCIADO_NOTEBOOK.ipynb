{"cells":[{"cell_type":"markdown","source":["# **INTELIGENCIA ARTIFICIAL APLICADA A LA CIBERSEGURIDAD**\n","## **PRÁCTICA P1 - BLOQUE I**\n","\n","**INSTRUCCIONES / RECOMENDACIONES**\n","\n","- Se recomienda leer con detalle la descripción de cada una de las celdas.\n","- Las celdas que ya tienen código, se deberán ejecutar directamente.\n","- Las celdas que están vacías, se completarán con la implementación requerida en el notebook.\n","- No se incluirán más celdas de las establecidas en el presente notebook, por lo que la solución al mismo deberá implementarse exclusivamente en las celdas vacías.\n","- Scikit-Learn es un paquete muy útil para las operaciones de preprocesamiento de los datos, como estandarización, normalización, codificación y performance de los modelos.\n","- Si ves que un apartado es complejo, intenta escribir y ejecutarlo de forma simplificada (por ejemplo, con menos layers o con menos features) y después vaya ampliándolo.\n","- La entrega se realizará vía Moodle. Será necesario subir la solución a este notebook con el nombre: **NOMBRE_GRUPO.ipynb**\n","\n","- **Fecha de Publicación: 12/02/2024**\n","- **Fecha de Entrega: 18/02/2024**\n","- **Test: 19/02/2024**\n"],"metadata":{"id":"XFJHhP0CNCq0"},"id":"XFJHhP0CNCq0"},{"cell_type":"markdown","source":["# Carga de librerías"],"metadata":{"id":"6SPLwA4uN8oP"},"id":"6SPLwA4uN8oP"},{"cell_type":"code","execution_count":null,"id":"731a1aa0-a5ac-4398-a5a0-834a71fd0e17","metadata":{"id":"731a1aa0-a5ac-4398-a5a0-834a71fd0e17"},"outputs":[],"source":["import os\n","import torch\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","print(torch.__version__)"]},{"cell_type":"markdown","source":["# Carga dataset\n","\n","Se va a usar un dataset ('**creditcard.csv**') que contiene información sobre transacciones económicas, en donde cada una de las transacciones está etiquetada como caso de fraude o caso de no fraude.\n","El objetivo será construir un modelo MLP que permita detectar transacciones fraudulentas, tratándose por lo tanto de un problema de clasificación binaria.\n","\n","El dataset se puede descargar desde el siguiente enlace:\n","https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download\n"],"metadata":{"id":"hESY4dJdOanL"},"id":"hESY4dJdOanL"},{"cell_type":"code","execution_count":null,"id":"b33f2378-9189-424f-9bc7-5d4bcfdb8358","metadata":{"id":"b33f2378-9189-424f-9bc7-5d4bcfdb8358"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Preparación de los dataset de Train, Validation y Test\n","\n","Se deberán crear los dataset **train_loader**, **val_loader** y **test_loader** (Entrenamiento, validación y test), especificando el parámetro **batch_size** que se empleará posteriormente durante el entrenamiento."],"metadata":{"id":"Mj7hRdl6PmmE"},"id":"Mj7hRdl6PmmE"},{"cell_type":"code","execution_count":null,"id":"a9d5b021-9800-4976-9416-680beed2515e","metadata":{"id":"a9d5b021-9800-4976-9416-680beed2515e"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Análisis del dataset\n","\n","Realice un análisis de las variables en el dataset.\n","Sugerencias:\n","- Histogramas de las variables.\n","- Obtención de valores de centralidad y dispersión.\n","- Detectar variables que no parezcan predecir el target o si presentan alta correlación con alguna otra de las features.\n","\n"],"metadata":{"id":"FwtMMeEKSymt"},"id":"FwtMMeEKSymt"},{"cell_type":"code","source":[],"metadata":{"id":"x7E6AgtBTAII"},"id":"x7E6AgtBTAII","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8xwJ1lBhTEDS"},"id":"8xwJ1lBhTEDS","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xMXriuIITA3B"},"id":"xMXriuIITA3B","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Activación de GPU"],"metadata":{"id":"8hfU5X-MQEv8"},"id":"8hfU5X-MQEv8"},{"cell_type":"code","execution_count":null,"id":"ecd2813b-1e3b-49b1-bf9d-93103b0cd843","metadata":{"id":"ecd2813b-1e3b-49b1-bf9d-93103b0cd843"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","torch.cuda.get_device_name(0)"]},{"cell_type":"markdown","source":["# Creación del modelo MLP\n","\n","Se creará un modelo MLP denominado **model** que implementará un clasificador binario para la detección de transacciones fraudulentas."],"metadata":{"id":"k1zkZdxDQOC3"},"id":"k1zkZdxDQOC3"},{"cell_type":"code","execution_count":null,"id":"274dd20d-48ab-41da-918c-0020a6a47832","metadata":{"id":"274dd20d-48ab-41da-918c-0020a6a47832"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Definición de función de pérdida y optimizador\n","\n","Especificar el parámetro **lr** (learning rate) que se empleará durante el entrenamiento del modelo."],"metadata":{"id":"HXZ9vpdDQkEY"},"id":"HXZ9vpdDQkEY"},{"cell_type":"code","execution_count":null,"id":"42be73e8-762e-4d90-a746-b52e9bb2163d","metadata":{"id":"42be73e8-762e-4d90-a746-b52e9bb2163d"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Definición de funciones de Entrenamiento y Validación"],"metadata":{"id":"sidnVK-xQxh-"},"id":"sidnVK-xQxh-"},{"cell_type":"code","execution_count":null,"id":"2233e97b-ddf5-48d3-85a9-9c669089886b","metadata":{"id":"2233e97b-ddf5-48d3-85a9-9c669089886b"},"outputs":[],"source":["def train_model(model, train_loader, criterion, optimizer, epoch):\n","\n","    model.train()\n","\n","    total_epoch_loss = 0\n","    total_epoch_acc = 0\n","\n","    for i, (X, Y) in enumerate(train_loader):\n","\n","        X, Y = X.to(device), Y.to(device)\n","\n","        outputs = model(X)\n","        loss = criterion(outputs, Y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        num_corrects = (torch.max(outputs, 1)[1].view(Y.size()).data == Y.data).float().sum()\n","        acc = 100.0 * num_corrects/len(Y)\n","\n","        if (i+1) % 1000 == 0:\n","          print (f'Epoch: {epoch+1}, Idx: ({i+1}/{len(train_loader)}), Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n","\n","        total_epoch_loss += loss.item()\n","        total_epoch_acc += acc.item()\n","\n","    return total_epoch_loss/len(train_loader), total_epoch_acc/len(train_loader)\n","\n","def eval_model(model, val_loader, criterion):\n","\n","  model.eval()\n","\n","  total_epoch_loss = 0\n","  total_epoch_acc = 0\n","\n","  with torch.no_grad():\n","\n","    for i, (X, Y) in enumerate(val_loader):\n","\n","        X, Y = X.to(device), Y.to(device)\n","\n","        outputs = model(X)\n","        loss = criterion(outputs, Y)\n","\n","        num_corrects = (torch.max(outputs, 1)[1].view(Y.size()).data == Y.data).float().sum()\n","        acc = 100.0 * num_corrects/len(Y)\n","\n","        total_epoch_loss += loss.item()\n","        total_epoch_acc += acc.item()\n","\n","    return total_epoch_loss/len(val_loader), total_epoch_acc/len(val_loader)"]},{"cell_type":"markdown","source":["# Entrenamiento del modelo\n","\n","Especificar el parámetro **epochs** que se utilizará durante el entrenamiento del modelo."],"metadata":{"id":"tKuZg3k2RGAD"},"id":"tKuZg3k2RGAD"},{"cell_type":"code","source":[],"metadata":{"id":"J8nUa81hRyze"},"id":"J8nUa81hRyze","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"9fc99b0a-77b6-4210-abab-d76ba93e3dee","metadata":{"id":"9fc99b0a-77b6-4210-abab-d76ba93e3dee"},"outputs":[],"source":["train_loss_epochs = []\n","val_loss_epochs = []\n","train_acc_epochs = []\n","val_acc_epochs = []\n","\n","for epoch in range(epochs):\n","    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, epoch)\n","    val_loss, val_acc = eval_model(model, val_loader, criterion)\n","\n","    print(f'Epoch: [{epoch+1:02}/{epochs}], Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n","    train_loss_epochs.append(train_loss)\n","    val_loss_epochs.append(val_loss)\n","    train_acc_epochs.append(train_acc)\n","    val_acc_epochs.append(val_acc)"]},{"cell_type":"markdown","source":["# Mostrar la evolución del error y del accuracy durante el entrenamiento del modelo\n","\n","Para mostrar el error y accuracy del dataset de train y de val, haga uso de las listas creadas durante el entrenamiento del modelo (ver celda anterior)."],"metadata":{"id":"bqd2c7G2ST12"},"id":"bqd2c7G2ST12"},{"cell_type":"code","source":[],"metadata":{"id":"ExYVBlElSZPy"},"id":"ExYVBlElSZPy","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oXWHHrqZSnbA"},"id":"oXWHHrqZSnbA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Verificar el rendimiento del modelo creado\n","\n","Utilice el dataset de test para verificar el correcto rendimiento del modelo. Para ello puede emplear las métricas que considere oportunas, como por ejemplo Accuracy, Precision, Recall, F1-score o matriz de confusión."],"metadata":{"id":"w9g--JPGSbR-"},"id":"w9g--JPGSbR-"},{"cell_type":"code","execution_count":null,"id":"3dcc272a-434c-4169-b5fd-a12a93b93407","metadata":{"id":"3dcc272a-434c-4169-b5fd-a12a93b93407"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"c488a279-85fb-4e73-b11e-bb6cdb49ca69","metadata":{"id":"c488a279-85fb-4e73-b11e-bb6cdb49ca69"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"GSuDPWOeSk5f"},"id":"GSuDPWOeSk5f","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}